{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wNTf8jfdzfLn",
        "outputId": "903f945c-89d1-4010-f15d-f7acf99bb2af"
      },
      "outputs": [],
      "source": [
        "#!pip install demoji"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [],
      "source": [
        "#!pip install langdetect"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4qtvmn6By3zR",
        "outputId": "1fab1c33-a505-4f8f-b08b-93092d6a7367"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\98936\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\98936\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package genesis to\n",
            "[nltk_data]     C:\\Users\\98936\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package genesis is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     C:\\Users\\98936\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\98936\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package brown to\n",
            "[nltk_data]     C:\\Users\\98936\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 57,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import gzip\n",
        "import nltk\n",
        "import spacy\n",
        "import string\n",
        "import re\n",
        "import warnings\n",
        "import demoji\n",
        "\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.collocations import *\n",
        "from collections import Counter\n",
        "from nltk.stem import porter\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "import re\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('genesis')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "stemmer = porter.PorterStemmer()\n",
        "\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "warnings.simplefilter(action='ignore', category=DeprecationWarning)\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('brown')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "l9UxRvKry3zS"
      },
      "outputs": [],
      "source": [
        "# Importing the dataset\n",
        "url = 'https://raw.githubusercontent.com/shahrzadko/NLPDigitalMusic/main/Digital_Music_5.json?token=GHSAT0AAAAAAB6LOL42SKHPCCMIVYPOC4REY7AKE2Q'\n",
        "df_raw = pd.read_json(url)\n",
        "full_df = pd.DataFrame.from_records(df_raw['data'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "qlniHtyhy3zT"
      },
      "outputs": [],
      "source": [
        "#creating Sentiment column with overall rating\n",
        "full_df['Sentiment'] = np.where((full_df['overall'] > 3), 'Positive', \n",
        "                          np.where((full_df['overall'] < 3), 'Negative', 'Neutral'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bgWLiGKQy3zT",
        "outputId": "0eeaa003-2ef4-4bb6-fa1f-1534d75da0ea"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Positive    158985\n",
              "Neutral       6792\n",
              "Negative      4004\n",
              "Name: Sentiment, dtype: int64"
            ]
          },
          "execution_count": 60,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#showing that the data is not balanced\n",
        "full_df['Sentiment'].value_counts() "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "2APJ3Sgmy3zT"
      },
      "outputs": [],
      "source": [
        "#creating  'NewReview'  column using  'reviewText' and 'summary' columns\n",
        "cols = ['reviewText','summary']\n",
        "full_df['NewReview'] = full_df[cols].apply(lambda row: ' '.join(row.values.astype(str)), axis=1)\n",
        "full_df['NewReview'] = full_df['NewReview'].astype(str)\n",
        "full_df['review_length'] = full_df['NewReview'].apply(lambda x: len(x.split()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "2Ru99Y21y3zU"
      },
      "outputs": [],
      "source": [
        "music_df_balanced = full_df.drop(columns=['vote','image','reviewTime','style','reviewerName','unixReviewTime','summary','reviewText'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "IJQxn9y6y3zU"
      },
      "outputs": [],
      "source": [
        "#dropping rows if the review is not verified\n",
        "music_df_balanced = music_df_balanced[music_df_balanced.verified == True]\n",
        "\n",
        "#dropping rows if the reviewer generally gives very negative reviews\n",
        "df_filtered = music_df_balanced.groupby('reviewerID').filter(lambda x: not((x['overall'].count() >= 10) and (x['overall'].mean() <= 2)))\n",
        "\n",
        "#dropping rows if the review is too short\n",
        "#df_filtered = df_filtered[df_filtered['review_length'] >= 6]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1pXAlTtDy3zU",
        "outputId": "a8f8b04d-8322-430a-b7e1-59e5b12136bb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Positive    141104\n",
              "Neutral       5396\n",
              "Negative      2093\n",
              "Name: Sentiment, dtype: int64"
            ]
          },
          "execution_count": 64,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#checking the values of Sentiment to creat a sample data\n",
        "df_filtered['Sentiment'].value_counts() "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "r9dZIepMy3zU"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "\n",
        "# Reduce the number of positive sentiment rows to 5000\n",
        "df_pos = df_filtered[df_filtered['Sentiment'] == 'Positive'].sample(n=10000, random_state=42)\n",
        "\n",
        "# Split the remaining dataset by sentiment\n",
        "df_neutral = df_filtered[df_filtered['Sentiment'] == 'Neutral']\n",
        "df_neg = df_filtered[df_filtered['Sentiment'] == 'Negative']\n",
        "\n",
        "# Define function for synonym replacement\n",
        "def synonym_replacement(text):\n",
        "    words = nltk.word_tokenize(text)\n",
        "    new_words = []\n",
        "    for word in words:\n",
        "        synonyms = []\n",
        "        for syn in wordnet.synsets(word):\n",
        "            for lemma in syn.lemmas():\n",
        "                synonyms.append(lemma.name())\n",
        "        if synonyms:\n",
        "            new_word = synonyms[0]\n",
        "        else:\n",
        "            new_word = word\n",
        "        new_words.append(new_word)\n",
        "    new_text = ' '.join(new_words)\n",
        "    return new_text\n",
        "\n",
        "# Oversample minority classes using synonym replacement\n",
        "desired_balance = 0.9\n",
        "while len(df_neutral) / len(df_pos) < desired_balance or len(df_neg) / len(df_pos) < desired_balance:\n",
        "    if len(df_neutral) / len(df_pos) < desired_balance:\n",
        "        sample = df_neutral.sample(n=1)['NewReview'].iloc[0]\n",
        "        new_review = synonym_replacement(sample)\n",
        "        df_neutral = df_neutral.append({'NewReview': new_review, 'Sentiment': 'Neutral'}, ignore_index=True)\n",
        "    if len(df_neg) / len(df_pos) < desired_balance:\n",
        "        sample = df_neg.sample(n=1)['NewReview'].iloc[0]\n",
        "        new_review = synonym_replacement(sample)\n",
        "        df_neg = df_neg.append({'NewReview': new_review, 'Sentiment': 'Negative'}, ignore_index=True)\n",
        "\n",
        "\n",
        "# Combine oversampled subsets back into a single dataframe\n",
        "df_balanced = pd.concat([df_pos, df_neutral, df_neg], ignore_index=True)\n",
        "\n",
        "# Shuffle the dataframe\n",
        "df_balanced = df_balanced.sample(frac=1).reset_index(drop=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZxpJ0RaWy3zV",
        "outputId": "17801330-0e65-4145-b4f2-f1236be03c0d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Positive    10000\n",
              "Neutral      9000\n",
              "Negative     9000\n",
              "Name: Sentiment, dtype: int64"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_balanced['Sentiment'].value_counts() "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "1YxKXAcQy3zV"
      },
      "outputs": [],
      "source": [
        "#creating balanced data using equal number of each sentiment\n",
        "# sample_size = min(df_filtered['Sentiment'].value_counts())\n",
        "# df_balanced = df_filtered.groupby('Sentiment').apply(lambda x: x.sample(sample_size)).reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "2vYRmL6Xy3zV"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Negative    2093\n",
              "Neutral     2093\n",
              "Positive    2093\n",
              "Name: Sentiment, dtype: int64"
            ]
          },
          "execution_count": 66,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_balanced['Sentiment'].value_counts() "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "Jh9VVfP6y3zW",
        "outputId": "50fa18c2-d443-4f9d-e123-e14686574895"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>overall</th>\n",
              "      <th>verified</th>\n",
              "      <th>reviewerID</th>\n",
              "      <th>asin</th>\n",
              "      <th>Sentiment</th>\n",
              "      <th>NewReview</th>\n",
              "      <th>review_length</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5.0</td>\n",
              "      <td>True</td>\n",
              "      <td>A12OSNTDE2U22V</td>\n",
              "      <td>B003DDY3NG</td>\n",
              "      <td>Positive</td>\n",
              "      <td>I love this song by Elvis !! It's sounds great...</td>\n",
              "      <td>15.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3.0</td>\n",
              "      <td>True</td>\n",
              "      <td>A2O1EBPCAGFQ88</td>\n",
              "      <td>B0011Z4Y6E</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Something a little different from a singer who...</td>\n",
              "      <td>37.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5.0</td>\n",
              "      <td>True</td>\n",
              "      <td>A27D4DN0N153XP</td>\n",
              "      <td>B000W1UAMA</td>\n",
              "      <td>Positive</td>\n",
              "      <td>Love Mary J! Five Stars</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Negative</td>\n",
              "      <td>This song beryllium manner excessively deceler...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Negative</td>\n",
              "      <td>iodine make not like this song , evening thoug...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   overall verified      reviewerID        asin Sentiment  \\\n",
              "0      5.0     True  A12OSNTDE2U22V  B003DDY3NG  Positive   \n",
              "1      3.0     True  A2O1EBPCAGFQ88  B0011Z4Y6E   Neutral   \n",
              "2      5.0     True  A27D4DN0N153XP  B000W1UAMA  Positive   \n",
              "3      NaN      NaN             NaN         NaN  Negative   \n",
              "4      NaN      NaN             NaN         NaN  Negative   \n",
              "\n",
              "                                           NewReview  review_length  \n",
              "0  I love this song by Elvis !! It's sounds great...           15.0  \n",
              "1  Something a little different from a singer who...           37.0  \n",
              "2                            Love Mary J! Five Stars            5.0  \n",
              "3  This song beryllium manner excessively deceler...            NaN  \n",
              "4  iodine make not like this song , evening thoug...            NaN  "
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_balanced.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "AhsLz4Suy3zW"
      },
      "outputs": [],
      "source": [
        "#drop non-English comments\n",
        "from langdetect import detect\n",
        "\n",
        "def is_english(text):\n",
        "    try:\n",
        "        lang = detect(text)\n",
        "        return lang == 'en'\n",
        "    except:\n",
        "        return False\n",
        "\n",
        "df_balanced = df_balanced[df_balanced['NewReview'].apply(is_english)]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "roWPpZmjy3zW",
        "outputId": "d31364f0-3066-41a2-8440-ad987e83ab02"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Positive    9462\n",
              "Neutral     8749\n",
              "Negative    8157\n",
              "Name: Sentiment, dtype: int64"
            ]
          },
          "execution_count": 52,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#checking the values of Sentiment after removing non-English reviews\n",
        "df_balanced['Sentiment'].value_counts() "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "_UINDW2Vy3zW"
      },
      "outputs": [],
      "source": [
        "df_balanced = df_balanced.drop(columns=['verified','overall','asin','reviewerID'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "sq5322KKy3zW",
        "outputId": "0182306a-fad8-4f9d-92bb-a75c5431271f"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>overall</th>\n",
              "      <th>Sentiment</th>\n",
              "      <th>NewReview</th>\n",
              "      <th>review_length</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.0</td>\n",
              "      <td>Negative</td>\n",
              "      <td>This is the worst sleep music I have purchased...</td>\n",
              "      <td>39</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2.0</td>\n",
              "      <td>Negative</td>\n",
              "      <td>Ambrosia Anthology had the same songs plus man...</td>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2.0</td>\n",
              "      <td>Negative</td>\n",
              "      <td>This was not a version that I expected. Maybe ...</td>\n",
              "      <td>41</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.0</td>\n",
              "      <td>Negative</td>\n",
              "      <td>Not what i expected at all. it was not my top ...</td>\n",
              "      <td>24</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.0</td>\n",
              "      <td>Negative</td>\n",
              "      <td>I was a fan of Marx years ago but wasn't a fan...</td>\n",
              "      <td>19</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   overall Sentiment                                          NewReview  \\\n",
              "0      1.0  Negative  This is the worst sleep music I have purchased...   \n",
              "1      2.0  Negative  Ambrosia Anthology had the same songs plus man...   \n",
              "2      2.0  Negative  This was not a version that I expected. Maybe ...   \n",
              "3      1.0  Negative  Not what i expected at all. it was not my top ...   \n",
              "4      1.0  Negative  I was a fan of Marx years ago but wasn't a fan...   \n",
              "\n",
              "   review_length  \n",
              "0             39  \n",
              "1             50  \n",
              "2             41  \n",
              "3             24  \n",
              "4             19  "
            ]
          },
          "execution_count": 68,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_balanced.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_paper_data = df_balanced"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "N8Zcubsgy3zc"
      },
      "outputs": [],
      "source": [
        "from nltk.stem import *\n",
        "\n",
        "# Initialize stopwords\n",
        "STOPWORDS = set(stopwords.words('english'))\n",
        "# Initialize stemmer and lemmatizer\n",
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Define function for text cleaning\n",
        "def clean_text(text):\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove URLs\n",
        "    text = re.sub(r'http\\S+', '', text)\n",
        "\n",
        "    # Remove currency symbols\n",
        "    text = re.sub(r'£|\\$', '', text)\n",
        "\n",
        "    # Remove phone numbers\n",
        "    text = re.sub(r'^\\(?[\\d]{3}\\)?[\\s-]?[\\d]{3}[\\s-]?[\\d]{4}$', '', text)\n",
        "\n",
        "    # Remove email addresses\n",
        "    text = re.sub(r'\\S+@\\S+', '', text)\n",
        "\n",
        "    # Remove punctuation and special characters except emojis\n",
        "    #text = re.sub(r'[^\\w\\s' + emoji.get_emoji_regexp() + ']', '', text)\n",
        "    text = demoji.replace(text, '')\n",
        "\n",
        "    # Tokenize text into words\n",
        "    words = nltk.word_tokenize(text)\n",
        "    \n",
        "    # Remove stopwords and handle negation\n",
        "    negation_words = ['not', 'no', 'never', 'neither', 'nor']\n",
        "    words = [word if word.lower() in negation_words else word.lower() for word in words]\n",
        "    words = [word for word in words if word.lower() not in STOPWORDS]\n",
        "    words = [f'NOT_{words[i+1]}' if (i < len(words)-1 and words[i].lower() in negation_words and words[i+1] not in string.punctuation) else words[i] for i in range(len(words))]\n",
        "    \n",
        "    # Lemmatize words\n",
        "    words = [lemmatizer.lemmatize(word) for word in words]\n",
        "\n",
        "    # Stem words\n",
        "    words = [stemmer.stem(word) for word in words]\n",
        "\n",
        "    # Join words back into text\n",
        "    cleaned_text = ' '.join(words)\n",
        "    return cleaned_text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "Fuhyn5Zvy3zc"
      },
      "outputs": [],
      "source": [
        "df_balanced['processed_review'] = df_balanced['NewReview'].apply(clean_text)\n",
        "# from multiprocessing import cpu_count, Pool\n",
        "# pool = Pool(cpu_count())\n",
        "# tasks = df_balanced['NewReview'].tolist()\n",
        "# done = pool.map(clean_text, tasks)  # parallel processing\n",
        "# df_balanced['processed_review'] = done"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "4gyvBEpZy3zd",
        "outputId": "80d56ee7-d73e-4a6b-ccde-a6bcd3d95ce2"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sentiment</th>\n",
              "      <th>NewReview</th>\n",
              "      <th>review_length</th>\n",
              "      <th>processed_review</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Positive</td>\n",
              "      <td>Real catchy Four Stars</td>\n",
              "      <td>4.0</td>\n",
              "      <td>real catchi four star</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Negative</td>\n",
              "      <td>This artist beryllium apparently very popular ...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>thi artist beryllium appar veri popular inch t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Positive</td>\n",
              "      <td>Cool Song, it brings back memories of when i w...</td>\n",
              "      <td>15.0</td>\n",
              "      <td>cool song , it bring back memori of when i wa ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Negative</td>\n",
              "      <td>iodine bargain this for my son . iodine americ...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>iodin bargain thi for my son . iodin americium...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Neutral</td>\n",
              "      <td>I have an older Decemberists CD and was happy ...</td>\n",
              "      <td>86.0</td>\n",
              "      <td>i have an older decemberist cd and wa happi th...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  Sentiment                                          NewReview  review_length  \\\n",
              "0  Positive                             Real catchy Four Stars            4.0   \n",
              "1  Negative  This artist beryllium apparently very popular ...            NaN   \n",
              "2  Positive  Cool Song, it brings back memories of when i w...           15.0   \n",
              "3  Negative  iodine bargain this for my son . iodine americ...            NaN   \n",
              "4   Neutral  I have an older Decemberists CD and was happy ...           86.0   \n",
              "\n",
              "                                    processed_review  \n",
              "0                              real catchi four star  \n",
              "1  thi artist beryllium appar veri popular inch t...  \n",
              "2  cool song , it bring back memori of when i wa ...  \n",
              "3  iodin bargain thi for my son . iodin americium...  \n",
              "4  i have an older decemberist cd and wa happi th...  "
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_balanced.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "Dq94fu0uy3zd"
      },
      "outputs": [],
      "source": [
        "df_balanced = df_balanced.drop(columns=['NewReview'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "cAwmKxNEy3zd",
        "outputId": "db3dd0f8-0747-4f02-9b68-95b985d31f1d"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sentiment</th>\n",
              "      <th>review_length</th>\n",
              "      <th>processed_review</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Positive</td>\n",
              "      <td>4.0</td>\n",
              "      <td>real catchi four star</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Negative</td>\n",
              "      <td>NaN</td>\n",
              "      <td>thi artist beryllium appar veri popular inch t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Positive</td>\n",
              "      <td>15.0</td>\n",
              "      <td>cool song , it bring back memori of when i wa ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Negative</td>\n",
              "      <td>NaN</td>\n",
              "      <td>iodin bargain thi for my son . iodin americium...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Neutral</td>\n",
              "      <td>86.0</td>\n",
              "      <td>i have an older decemberist cd and wa happi th...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  Sentiment  review_length                                   processed_review\n",
              "0  Positive            4.0                              real catchi four star\n",
              "1  Negative            NaN  thi artist beryllium appar veri popular inch t...\n",
              "2  Positive           15.0  cool song , it bring back memori of when i wa ...\n",
              "3  Negative            NaN  iodin bargain thi for my son . iodin americium...\n",
              "4   Neutral           86.0  i have an older decemberist cd and wa happi th..."
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_balanced.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "hYSMu5Qcy3zd"
      },
      "outputs": [],
      "source": [
        "X = pd.DataFrame(df_balanced['processed_review'])\n",
        "y = df_balanced['Sentiment']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qo4FDvHJy3ze"
      },
      "source": [
        "TfidfVectorizer / Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cwAiZzOvy3ze",
        "outputId": "b1815982-1618-4d24-8c6f-5c47b9c2345b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy for TFIDF Vectorizer and Naive Bayes: 0.852\n"
          ]
        }
      ],
      "source": [
        "# Split the data into training and testing sets with stratified sampling\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=42)\n",
        "\n",
        "# Define the pipeline\n",
        "pipeline = Pipeline([\n",
        "    ('tfidf', TfidfVectorizer()),\n",
        "    ('nb', MultinomialNB())\n",
        "])\n",
        "\n",
        "# Fit the pipeline on the training data\n",
        "pipeline.fit(X_train['processed_review'], y_train)\n",
        "\n",
        "# Predict on the test data\n",
        "y_pred = pipeline.predict(X_test['processed_review'])\n",
        "\n",
        "# Calculate the accuracy\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy for TFIDF Vectorizer and Naive Bayes: {acc:.3f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SEQbyh3JLJmD",
        "outputId": "919a461f-b995-48af-8cca-cc51ac252c42"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "confusion matrix and classification report for TFIDF Vectorizer and Naive Bayes\n",
            "[[2140  217  121]\n",
            " [ 343 1920  355]\n",
            " [  15  120 2702]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.86      0.86      0.86      2478\n",
            "     Neutral       0.85      0.73      0.79      2618\n",
            "    Positive       0.85      0.95      0.90      2837\n",
            "\n",
            "    accuracy                           0.85      7933\n",
            "   macro avg       0.85      0.85      0.85      7933\n",
            "weighted avg       0.85      0.85      0.85      7933\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print('confusion matrix and classification report for TFIDF Vectorizer and Naive Bayes')\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bB-6Es2Ty3zf"
      },
      "source": [
        "Count Vectorizer / Naive Bayse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Duq9-_8Py3zf",
        "outputId": "f95474a6-77da-4a56-feec-85d1f4681c92"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy Accuracy for Count Vectorizer and Naive Bayes: 0.8290684482541283\n"
          ]
        }
      ],
      "source": [
        "# Split the data into training and testing sets with stratified sampling\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=42)\n",
        " \n",
        "# Define CountVectorizer\n",
        "count_vectorizer = CountVectorizer()\n",
        "\n",
        "# Define Naive Bayes classifier\n",
        "nb_classifier = MultinomialNB()\n",
        "\n",
        "# Combine CountVectorizer and Naive Bayes classifier into a pipeline\n",
        "pipeline = Pipeline([\n",
        "    ('vectorizer', count_vectorizer),\n",
        "    ('classifier', nb_classifier)\n",
        "])\n",
        "\n",
        "# Fit the pipeline on the training data\n",
        "pipeline.fit(X_train['processed_review'], y_train)\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred = pipeline.predict(X_test['processed_review'])\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Accuracy Accuracy for Count Vectorizer and Naive Bayes: {accuracy}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HQegpZi7FFgJ",
        "outputId": "74a22a11-be4d-4dc0-d0d9-e3ed9162b6ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "confusion matrix and classification report for Count Vectorizer and Naive Bayes\n",
            "[[2047  257  174]\n",
            " [ 380 1815  423]\n",
            " [   9  113 2715]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.84      0.83      0.83      2478\n",
            "     Neutral       0.83      0.69      0.76      2618\n",
            "    Positive       0.82      0.96      0.88      2837\n",
            "\n",
            "    accuracy                           0.83      7933\n",
            "   macro avg       0.83      0.83      0.82      7933\n",
            "weighted avg       0.83      0.83      0.83      7933\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print('confusion matrix and classification report for Count Vectorizer and Naive Bayes')\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XHI467h8y3zf"
      },
      "source": [
        "#### Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QDAv5iAfy3zf",
        "outputId": "7af4936a-17c0-4536-e95e-c8bf468087e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy of Logistic Regression model: 0.8870540779024328\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\98936\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ],
      "source": [
        "# Split the data into training and testing sets with stratified sampling\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=42)\n",
        " \n",
        "# Define the pipeline\n",
        "model = make_pipeline(\n",
        "    CountVectorizer(),\n",
        "    StandardScaler(with_mean=False),\n",
        "    LogisticRegression(random_state=42)\n",
        ")\n",
        "\n",
        "# Fit the model to the training data\n",
        "model.fit(X_train['processed_review'], y_train)\n",
        "\n",
        "# Predict the sentiment of the test data\n",
        "y_pred = model.predict(X_test['processed_review'])\n",
        "\n",
        "# Evaluate the accuracy of the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy of Logistic Regression model: {accuracy}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZJBPz8ZSFG35",
        "outputId": "51c3b5ef-cab5-4a56-e3ba-a46f81bccd79"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "confusion matrix and classification report of Logistic Regression model\n",
            "[[2333   93   52]\n",
            " [ 122 2241  255]\n",
            " [  72  302 2463]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.92      0.94      0.93      2478\n",
            "     Neutral       0.85      0.86      0.85      2618\n",
            "    Positive       0.89      0.87      0.88      2837\n",
            "\n",
            "    accuracy                           0.89      7933\n",
            "   macro avg       0.89      0.89      0.89      7933\n",
            "weighted avg       0.89      0.89      0.89      7933\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print('confusion matrix and classification report of Logistic Regression model')\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oF2K3ARhy3zf"
      },
      "source": [
        "#### Gradient Boosting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tTAOvkWEy3zf",
        "outputId": "a9bf1f9d-986f-40c7-ccfe-9028d711f490"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy of Gradient Boosting: 0.841\n"
          ]
        }
      ],
      "source": [
        "# Split the data into training and testing sets with stratified sampling\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=42)\n",
        " \n",
        "# Define the pipeline\n",
        "pipeline = Pipeline([\n",
        "    ('tfidf', TfidfVectorizer()),\n",
        "    ('gb', GradientBoostingClassifier())\n",
        "])\n",
        "\n",
        "# Fit the pipeline on the training data\n",
        "pipeline.fit(X_train['processed_review'], y_train)\n",
        "\n",
        "# Predict on the test data\n",
        "y_pred = pipeline.predict(X_test['processed_review'])\n",
        "\n",
        "# Calculate the accuracy\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy of Gradient Boosting: {acc:.3f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tt1bsaBCFH0p",
        "outputId": "04770e2b-469e-434d-e5fc-1d505de86278"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "confusion matrix and classification report of Gradient Boosting\n",
            "[[2109  212  157]\n",
            " [ 349 1905  364]\n",
            " [  28  150 2659]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.85      0.85      0.85      2478\n",
            "     Neutral       0.84      0.73      0.78      2618\n",
            "    Positive       0.84      0.94      0.88      2837\n",
            "\n",
            "    accuracy                           0.84      7933\n",
            "   macro avg       0.84      0.84      0.84      7933\n",
            "weighted avg       0.84      0.84      0.84      7933\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print('confusion matrix and classification report of Gradient Boosting')\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qG9_dN6qCdod"
      },
      "source": [
        "SVM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mc8UvlRQy3zg",
        "outputId": "54a52072-b57b-465e-e152-05488d5d3c15"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy of SVM before tuning: 0.919\n"
          ]
        }
      ],
      "source": [
        "# Split the data into training and testing sets with stratified sampling\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=42)\n",
        " \n",
        "# Define the pipeline\n",
        "pipeline = Pipeline([\n",
        "    ('tfidf', TfidfVectorizer()),\n",
        "    ('svm', SVC())\n",
        "])\n",
        "\n",
        "# Fit the pipeline on the training data\n",
        "pipeline.fit(X_train['processed_review'], y_train)\n",
        "\n",
        "# Predict on the test data\n",
        "y_pred = pipeline.predict(X_test['processed_review'])\n",
        "\n",
        "# Calculate the accuracy\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy of SVM before tuning: {acc:.3f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "78DgfCdVFJSR",
        "outputId": "30ef7f15-875f-4096-d29b-55bced32cd9c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "confusion matrix and classification report of SVM before tuning\n",
            "[[2295  125   58]\n",
            " [  72 2306  240]\n",
            " [  16  134 2687]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.96      0.93      0.94      2478\n",
            "     Neutral       0.90      0.88      0.89      2618\n",
            "    Positive       0.90      0.95      0.92      2837\n",
            "\n",
            "    accuracy                           0.92      7933\n",
            "   macro avg       0.92      0.92      0.92      7933\n",
            "weighted avg       0.92      0.92      0.92      7933\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print('confusion matrix and classification report of SVM before tuning')\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "aFwENdary3zg"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\98936\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
            "300 fits failed out of a total of 500.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "100 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\98936\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"c:\\Users\\98936\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 394, in fit\n",
            "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
            "  File \"c:\\Users\\98936\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 177, in fit\n",
            "    raise TypeError(\"Sparse precomputed kernels are not supported.\")\n",
            "TypeError: Sparse precomputed kernels are not supported.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "100 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\98936\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"c:\\Users\\98936\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 394, in fit\n",
            "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
            "  File \"c:\\Users\\98936\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 243, in fit\n",
            "    raise ValueError(\n",
            "ValueError: When 'gamma' is a string, it should be either 'scale' or 'auto'. Got 'float' instead.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "100 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\98936\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"c:\\Users\\98936\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 394, in fit\n",
            "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
            "  File \"c:\\Users\\98936\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 243, in fit\n",
            "    raise ValueError(\n",
            "ValueError: When 'gamma' is a string, it should be either 'scale' or 'auto'. Got 'array-like' instead.\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
            "c:\\Users\\98936\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the test scores are non-finite: [0.84300378 0.82474338 0.83246894        nan 0.73009184 0.84300378\n",
            " 0.35759049 0.35759049        nan 0.35759049        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan 0.89005943 0.91269584 0.86591032        nan\n",
            " 0.90637493 0.89005943 0.35759049 0.35759049        nan 0.35759049\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan 0.89340897 0.92009724\n",
            " 0.82560778        nan 0.91172339 0.89340897 0.65694219 0.37120475\n",
            "        nan 0.35759049        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            " 0.87806591 0.92009724 0.81712588        nan 0.91193949 0.87806591\n",
            " 0.77233928 0.74689357        nan 0.35759049        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan 0.87444625 0.92009724 0.81118314        nan\n",
            " 0.91193949 0.87444625 0.80432199 0.7723933         nan 0.35759049\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan]\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best parameters:  {'svm__C': 100, 'svm__gamma': 'scale', 'svm__kernel': 'rbf'}\n",
            "Accuracy: 0.928\n"
          ]
        }
      ],
      "source": [
        "# # Split the data into training and testing sets with stratified sampling\n",
        "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=42)\n",
        " \n",
        "\n",
        "# # Define the pipeline\n",
        "# pipeline = Pipeline([\n",
        "#     ('tfidf', TfidfVectorizer()),\n",
        "#     ('svm', SVC())\n",
        "# ])\n",
        "\n",
        "\n",
        "# # Define the hyperparameter grid\n",
        "# param_grid = {\n",
        "#     'svm__C': [0.1, 1, 10, 100,200],\n",
        "#     'svm__kernel': ['linear', 'rbf', 'sigmoid','precomputed','poly'],\n",
        "#     'svm__gamma': ['scale', 'auto','float','array-like']\n",
        "# }\n",
        "\n",
        "\n",
        "# # Define the GridSearchCV object\n",
        "# grid_search = GridSearchCV(pipeline, param_grid, cv=5, n_jobs=-1, verbose=2)\n",
        "\n",
        "# # Fit the pipeline on the training data\n",
        "# grid_search.fit(X_train['processed_review'], y_train)\n",
        "\n",
        "# # Print the best hyperparameters\n",
        "# print(\"Best parameters: \", grid_search.best_params_)\n",
        "\n",
        "# # Predict on the test data using the best estimator\n",
        "# y_pred = grid_search.best_estimator_.predict(X_test['processed_review'])\n",
        "\n",
        "# # Calculate the accuracy\n",
        "# acc = accuracy_score(y_test, y_pred)\n",
        "# print(f\"Accuracy: {acc:.3f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QObqfMvCy3zg",
        "outputId": "7c03b83b-0b60-4698-dd47-d64ebf71b7db"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.928\n"
          ]
        }
      ],
      "source": [
        "# Split the data into training and testing sets with stratified sampling\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=42)\n",
        " \n",
        "# Define the pipeline\n",
        "pipeline = Pipeline([\n",
        "    ('tfidf', TfidfVectorizer()),\n",
        "    ('svm', SVC(C= 100, gamma= 'scale', kernel='rbf'))\n",
        "])\n",
        "\n",
        "# Fit the pipeline on the training data\n",
        "pipeline.fit(X_train['processed_review'], y_train)\n",
        "\n",
        "# Predict on the test data\n",
        "y_pred = pipeline.predict(X_test['processed_review'])\n",
        "\n",
        "# Calculate the accuracy\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {acc:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "confusion matrix and classification report of SVM after tuning\n",
            "[[2349   88   41]\n",
            " [  70 2341  207]\n",
            " [  18  151 2668]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.96      0.95      0.96      2478\n",
            "     Neutral       0.91      0.89      0.90      2618\n",
            "    Positive       0.91      0.94      0.93      2837\n",
            "\n",
            "    accuracy                           0.93      7933\n",
            "   macro avg       0.93      0.93      0.93      7933\n",
            "weighted avg       0.93      0.93      0.93      7933\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print('confusion matrix and classification report of SVM after tuning')\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "qWVU__5ky3zg"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.927\n"
          ]
        }
      ],
      "source": [
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "\n",
        "# Split the data into training and testing sets with stratified sampling\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=42)\n",
        "\n",
        "# Define the pipeline\n",
        "pipeline = Pipeline([\n",
        "    ('tfidf', TfidfVectorizer()),\n",
        "    ('svm', SVC(C= 100, gamma= 'scale', kernel='rbf', probability=True)) # set probability to True\n",
        "])\n",
        "\n",
        "# Wrap the pipeline in a calibrated classifier\n",
        "calibrated_pipeline = CalibratedClassifierCV(pipeline, method='sigmoid', cv=5) # you can choose different calibration methods\n",
        "\n",
        "# Fit the pipeline on the training data\n",
        "calibrated_pipeline.fit(X_train['processed_review'], y_train)\n",
        "\n",
        "# Predict on the test data\n",
        "y_pred = calibrated_pipeline.predict(X_test['processed_review'])\n",
        "\n",
        "# Calculate the accuracy\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {acc:.3f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "confusion matrix and classification report of SVM after calibration\n",
            "[[2339   89   50]\n",
            " [  69 2346  203]\n",
            " [  16  154 2667]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.96      0.94      0.95      2478\n",
            "     Neutral       0.91      0.90      0.90      2618\n",
            "    Positive       0.91      0.94      0.93      2837\n",
            "\n",
            "    accuracy                           0.93      7933\n",
            "   macro avg       0.93      0.93      0.93      7933\n",
            "weighted avg       0.93      0.93      0.93      7933\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print('confusion matrix and classification report of SVM after calibration')\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\98936\\AppData\\Local\\Temp\\ipykernel_10648\\4051919555.py:56: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df_paper_data['processed_review'] = df_paper_data['NewReview'].apply(clean_text)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy for TFIDF Vectorizer and Naive Bayes: 0.620\n",
            "[[352 171  48]\n",
            " [131 387  91]\n",
            " [ 68 162 357]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.64      0.62      0.63       571\n",
            "     Neutral       0.54      0.64      0.58       609\n",
            "    Positive       0.72      0.61      0.66       587\n",
            "\n",
            "    accuracy                           0.62      1767\n",
            "   macro avg       0.63      0.62      0.62      1767\n",
            "weighted avg       0.63      0.62      0.62      1767\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\98936\\AppData\\Local\\Temp\\ipykernel_10648\\4051919555.py:57: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df_paper_data['processed_review'] = df_paper_data['processed_review'].apply(extract_opinion_words)\n",
            "C:\\Users\\98936\\AppData\\Local\\Temp\\ipykernel_10648\\4051919555.py:58: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df_paper_data[\"processed_review\"] = df_paper_data[\"processed_review\"].apply(lambda x: \" \".join(x))\n"
          ]
        }
      ],
      "source": [
        "def extract_opinion_words(review):\n",
        "    words = word_tokenize(review.lower())\n",
        "    pos_tags = nltk.pos_tag(words)\n",
        "    opinion_words = []\n",
        "    for word, tag in pos_tags:\n",
        "        if tag.startswith('JJ') or tag.startswith('VB'):\n",
        "            opinion_words.append(word)\n",
        "    return opinion_words\n",
        "# Initialize stopwords\n",
        "STOPWORDS = set(stopwords.words('english'))\n",
        "# Initialize stemmer and lemmatizer\n",
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Define function for text cleaning\n",
        "def clean_text(text):\n",
        "    # # Convert to lowercase\n",
        "    # text = text.lower()\n",
        "\n",
        "    # # Remove URLs\n",
        "    # text = re.sub(r'http\\S+', '', text)\n",
        "\n",
        "    # # Remove currency symbols\n",
        "    # text = re.sub(r'£|\\$', '', text)\n",
        "\n",
        "    # # Remove phone numbers\n",
        "    # text = re.sub(r'^\\(?[\\d]{3}\\)?[\\s-]?[\\d]{3}[\\s-]?[\\d]{4}$', '', text)\n",
        "\n",
        "    # # Remove digits from NewReview\n",
        "    # df_balanced['NewReview'] = df_balanced['NewReview'].apply(lambda x: re.sub(r'\\d+', '', x))\n",
        "\n",
        "    # # Remove email addresses\n",
        "    # text = re.sub(r'\\S+@\\S+', '', text)\n",
        "\n",
        "    # # Remove punctuation and special characters except emojis\n",
        "    # text = re.sub(r'[^\\w\\s'  + ']', '', text)\n",
        "\n",
        "    # Tokenize text into words\n",
        "    words = nltk.word_tokenize(text)\n",
        "    \n",
        "    # # Remove stopwords and handle negation\n",
        "    # negation_words = ['not', 'no', 'never', 'neither', 'nor']\n",
        "    # words = [word if word.lower() in negation_words else word.lower() for word in words]\n",
        "    # words = [word for word in words if word.lower() not in STOPWORDS]\n",
        "    # words = [f'NOT_{words[i+1]}' if (i < len(words)-1 and words[i].lower() in negation_words and words[i+1] not in string.punctuation) else words[i] for i in range(len(words))]\n",
        "    \n",
        "    # Lemmatize words\n",
        "    words = [lemmatizer.lemmatize(word) for word in words]\n",
        "\n",
        "    # Stem words\n",
        "    words = [stemmer.stem(word) for word in words]\n",
        "\n",
        "    # Join words back into text\n",
        "    cleaned_text = ' '.join(words)\n",
        "    return cleaned_text\n",
        "df_paper_data['processed_review'] = df_paper_data['NewReview'].apply(clean_text)\n",
        "df_paper_data['processed_review'] = df_paper_data['processed_review'].apply(extract_opinion_words)\n",
        "df_paper_data[\"processed_review\"] = df_paper_data[\"processed_review\"].apply(lambda x: \" \".join(x))\n",
        "X_paper = pd.DataFrame(df_paper_data['processed_review'])\n",
        "y_paper = df_paper_data['Sentiment']\n",
        "# Split the data into training and testing sets with stratified sampling\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_paper, y_paper, test_size=0.3, stratify=y_paper, random_state=42)\n",
        "\n",
        "# Define the pipeline\n",
        "pipeline = Pipeline([\n",
        "    ('tfidf', TfidfVectorizer()),\n",
        "    ('nb', MultinomialNB())\n",
        "])\n",
        "\n",
        "# Fit the pipeline on the training data\n",
        "pipeline.fit(X_train['processed_review'], y_train)\n",
        "\n",
        "# Predict on the test data\n",
        "y_pred = pipeline.predict(X_test['processed_review'])\n",
        "\n",
        "# Calculate the accuracy\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy for TFIDF Vectorizer and Naive Bayes: {acc:.3f}\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
